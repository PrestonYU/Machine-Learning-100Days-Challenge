# Machine Learning 100 Days Challenge 
Cupoy 3rd-ML-100Days


### 機器學習概論
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_001_HW.ipynb>Day1_資料介紹與評估資料 (申論+程式碼)</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_002_HW.ipynb>Day2_機器學習概論 (申論題)</a>** : 機器學習、深度學習與人工智慧差別是甚麼? 機器學習又有甚麼主題應用?
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_003_HW.ipynb>Day3_機器學習-流程與步驟 (申論題)</a>** : 資料前處理 > 訓練/測試集切分 >選定目標與評估基準 > 建立模型 > 調整參數。熟悉整個 ML 的流程
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_004_HW.ipynb>Day4_EDA/讀取資料與分析流程</a>** : 如何讀取資料以及萃取出想要了解的信息


### 資料清理數據前處理
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_005-1_HW.ipynb>Day5_如何新建一個 dataframe? 如何讀取其他資料? (非 csv 的資料)</a>** : 1. 從頭建立一個 dataframe 2. 如何讀取不同形式的資料 (如圖檔、純文字檔、json 等)
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_006_HW.ipynb>Day6_EDA: 欄位的資料類型介紹及處理</a>** : 了解資料在 pandas 中可以表示的類型
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_007_HW.ipynb>Day7_特徵類型</a>** : 特徵工程依照特徵類型，做法不同，大致可分為數值/類別/時間型三類特徵
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_008_HW.ipynb>Day8_EDA資料分佈</a>** : 用統計方式描述資料
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_009_HW.ipynb>Day9_EDA: Outlier 及處理</a>** : 偵測與處理例外數值點：1. 透過常用的偵測方法找到例外 2. 判斷例外是否正常 (推測可能的發生原因)
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_010_HW.ipynb>Day10_數值型特徵 - 去除離群值</a>** : 數值型特徵若出現少量的離群值，則需要去除以保持其餘數據不被影響
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_011_HW.ipynb>Day11_常用的數值取代</a>**：中位數與分位數連續數值標準化 偵測與處理例外數值 1. 缺值或例外取代 2. 數據標準化
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_012_HW.ipynb>Day12_數值型特徵-補缺失值與標準化</a>** : 數值型特徵首先必須填補缺值與標準化
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_013_HW.ipynb>Day13_DataFrame operation/Data frame merge常用的 DataFrame 操作</a>** : 1 常見的資料操作方法 2. 資料表串接
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_014_HW.ipynb>Day14_EDA: correlation/相關係數簡介</a>** : 1 了解相關係數 2. 利用相關係數直觀地理解對欄位與預測目標之間的關係
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_015_HW.ipynb>Day15_EDA from Correlation</a>** : 深入了解資料，從 correlation 的結果下手
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_016_HW.ipynb>Day16_EDA: 不同數值範圍間的特徵如何檢視/繪圖與樣式Kernel Density Estimation (KDE)</a>** : 如何調整視覺化方式檢視數值範圍、轉換繪圖樣式
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_017_HW.ipynb>Day17_EDA: 把連續型變數離散化</a>** : 簡化連續性變數
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_018_HW.ipynb>Day18_把連續型變數離散化</a>** : 深入了解資料，從簡化後的離散變數下手
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_019_HW.ipynb>Day19_Subplots 探索性資料分析 - 資料視覺化 - 多圖檢視</a>** : 1. 將數據分組一次呈現 2. 把同一組資料相關的數據一次攤在面前
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_020_HW.ipynb>Day20_Heatmap & Grid-plot 探索性資料分析 - 資料視覺化 - 熱像圖 / 格狀圖</a>** : 1. 熱圖：以直觀的方式檢視變數間的相關性 2. 格圖：繪製變數間的散佈圖及分布
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_021_HW.ipynb>Day21_模型初體驗 Logistic Regression</a>** : 


### 資料科學特徵工程技術
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_022_HW.ipynb>Day22_特徵工程簡介</a>** : 介紹機器學習完整步驟中，特徵工程的位置以及流程架構
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_023_HW.ipynb>Day23_數值型特徵 - 去除偏態</a>** : 數值型特徵若分布明顯偏一邊，則需去除偏態以消除預測的偏差
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_024_HW.ipynb>Day24_類別型特徵 - 基礎處理</a>** : 介紹類別型特徵最基礎的作法 : 標籤編碼與獨熱編碼
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_025_HW.ipynb>Day25_類別型特徵 - 均值編碼</a>** : 類別型特徵最重要的編碼 : 均值編碼，將標籤以目標均值取代
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_026_HW.ipynb>Day26_類別型特徵 - 其他進階處理</a>** : 類別型特徵的其他常見編碼 : 計數編碼對應出現頻率相關的特徵，雜湊編碼對應眾多類別而無法排序的特徵
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_027_HW.ipynb>Day27_時間型特徵</a>** : 時間型特徵可抽取出多個子特徵，或周期化，或取出連續時段內的次數
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_028_HW.ipynb>Day28_特徵組合 - 數值與數值組合</a>** : 特徵組合的基礎 : 以四則運算的各種方式，組合成更具預測力的特徵
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_029_HW.ipynb>Day29_特徵組合 - 類別與數值組合</a>** : 類別型對數值型特徵可以做群聚編碼，與目標均值編碼類似，但用途不同
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_030_HW.ipynb>Day30_特徵選擇</a>** : 介紹常見的幾種特徵篩選方式
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_031_HW.ipynb>Day31_特徵評估</a>** : 介紹並比較兩種重要的特徵評估方式，協助檢測特徵的重要性
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_032_HW.ipynb>Day32_分類型特徵優化 - 葉編碼</a>** : 葉編碼 : 適用於分類問題的樹狀預估模型改良


### 機器學習基礎模型建立
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_033_HW.ipynb>Day33_機器如何學習?</a>** : 了解機器學習的定義，過擬合 (Overfit) 是甚麼，該如何解決
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_034_HW.ipynb>Day34_訓練/測試集切分的概念</a>** : 為何要做訓練/測試集切分？有什麼切分的方法？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_035_HW.ipynb>Day35_Regression vs Classification</a>** : 回歸問題與分類問題的區別？如何定義專案的目標
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_036_HW.ipynb>Day36_評估指標選定/evaluation metrics</a>** : 專案該如何選擇評估指標？常用指標有哪些？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_037_HW.ipynb>Day37_Regression model 介紹 : - 線性迴歸/羅吉斯回歸 線性迴歸/羅吉斯回歸模型的理論基礎與使用時的注意事項</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_038_HW.ipynb>Day38_Regression model 程式碼撰寫</a>** : 如何使用 Scikit-learn 撰寫線性迴歸/羅吉斯回歸模型的程式碼
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_039_HW.ipynb>Day39_Regression model 介紹 - LASSO 回歸/ Ridge 回歸 LASSO 回歸/ Ridge 回歸的理論基礎與與使用時的注意事項</a>** 
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_040_HW.ipynb>Day40_Regression model 程式碼撰寫</a>** : 使用 Scikit-learn 撰寫 LASSO 回歸/ Ridge 回歸模型的程式碼
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_041_HW.ipynb>Day41_Tree-based model - 決策樹 (Decision Tree) 模型介紹</a>** : 決策樹 (Decision Tree) 模型的理論基礎與使用時的注意事項
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_042_HW.ipynb>Day42_Tree-based model - 決策樹程式碼撰寫</a>** : 使用 Scikit-learn 撰寫決策樹 (Decision Tree) 模型的程式碼
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_043_HW.ipynb>Day43_Tree-based model - 隨機森林 (Random Forest) 介紹</a>** : 隨機森林 (Random Forest)模型的理論基礎與使用時的注意事項
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_044_HW.ipynb>Day44_Tree-based model - 隨機森林程式碼撰寫</a>** : 使用 Scikit-learn 撰寫隨機森林 (Random Forest) 模型的程式碼
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_045_HW.ipynb>Day45_Tree-based model - 梯度提升機 (Gradient Boosting Machine) 介紹</a>** : 梯度提升機 (Gradient Boosting Machine) 模型的理論基礎與使用時的注意事項
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_046_HW.ipynb>Day46_Tree-based model - 梯度提升機程式碼撰寫</a>** : 使用 Scikit-learn 撰寫梯度提升機 (Gradient Boosting Machine) 模型的程式碼


### 機器學習調整參數
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_047_HW.ipynb>Day47_超參數調整與優化</a>** : 什麼是超參數 (Hyper-paramter) ? 如何正確的調整超參數？常用的調參方法為何？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_048_HW.ipynb>Day48_Kaggle 競賽平台介紹</a>** : 
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_049_Blending_HW.ipynb>Day49_集成方法 : 混合泛化(Blending)</a>** : 什麼是集成? 集成方法有哪些? Blending 的寫作方法與效果為何?
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_050_Stacking_HW.ipynb>Day50_集成方法 : 堆疊泛化(Stacking)</a>** 
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_051_053_HW.ipynb>Day51-53_Kaggle 第一次期中考</a>**


### 非監督式機器學習
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_054_HW.ipynb>Day54_Clustering 1 非監督式機器學習簡介</a>** : 非監督式學習簡介、應用場景
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_055_HW.ipynb>Day55_Clustering 2 聚類算法 K-means</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_056_kmean_HW.ipynb>Day56_K-mean 觀察</a>** : 使用輪廓分析 非監督模型要以特殊評估方法(而非評估函數)來衡量, 今日介紹大家了解並使用其中一種方法 : 輪廓分析
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_057_HW.ipynb>Day57_Clustering 3 階層分群算法 hierarchical clustering</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_058_hierarchical_clustering_HW.ipynb>Day58_階層分群法 觀察</a>** : 使用 2D 樣版資料集 非監督評估方法 : 2D樣版資料集是什麼? 如何生成與使用?
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_059_HW.ipynb>Day59_Dimension reduction 1 降維方法-主成份分析 PCA</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_060_PCA_HW.ipynb>Day60_PCA 觀察</a>** : 使用手寫辨識資料集 以較複雜的範例 : sklearn版手寫辨識資料集, 展示PCA的降維與資料解釋能力
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_061_HW.ipynb>Day61_Dimension reduction 2 降維方法-T-SNE TSNE</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_062_tsne_HW.ipynb>Day62_T-SNE 觀察</a>** : 分群與流形還原 什麼是流形還原? 除了 t-sne 之外還有那些常見的流形還原方法?


### 深度學習理論與實作
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_063_HW.ipynb>Day63_神經網路介紹</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_064_HW.ipynb>Day64_深度學習體驗 : 模型調整與學習曲線</a>** : 介紹體驗平台 TensorFlow PlayGround，並初步了解模型的調整
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day_065_HW.ipynb>Day65_深度學習體驗 : 啟動函數與正規化</a>** : 在 TF PlayGround 上，體驗進階版的深度學習參數調整


### 初探深度學習使用Keras
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day66-Keras_Introduction_HW.ipynb>Day66_Keras 安裝與介紹</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day67-Keras_Dataset_HW.ipynb>Day67_Keras Dataset</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day68-Keras_Sequential_Model_HW.ipynb>Day68_Keras Sequential API</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day69-keras_Module_API_HW.ipynb>Day79_Keras Module API</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day70-Keras_Mnist_MLP_HW.ipynb>Day70_Multi-layer Perception多層感知 MLP簡介</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day71-Loss Function_HW.ipynb>Day71_損失函數</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day72-Activation_function_HW.ipynb>Day72_啟動函數</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day73_Gradient_Descent_HW.ipynb>Day73_梯度下降Gradient Descent</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day74-Gradient_Descent_HW.ipynb>Day74_Gradient Descent 數學原理</a>** : 介紹梯度下降的基礎數學原理
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day75-Back_Propagation_HW.ipynb>Day75_BackPropagation 反向式傳播簡介</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/D76-optimizer_HW.ipynb>Day76_優化器Optimizers</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day077_HW.ipynb>Day77_訓練神經網路的細節與技巧 - Validation and overfit</a>** : 檢視並了解 overfit 現象
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day078_HW.ipynb>Day78_訓練神經網路前的注意事項</a>** : 資料是否經過妥善的處理？運算資源為何？超參數的設置是否正確？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day079_HW.ipynb>Day79_訓練神經網路的細節與技巧 - Learning rate effect 比較不同</a>** : Learning rate 對訓練過程及結果的差異
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day080_HW.ipynb>Day80_優化器與學習率的組合與比較</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day081_HW.ipynb>Day81_訓練神經網路的細節與技巧 - Regularization</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day082_HW.ipynb>Day82_訓練神經網路的細節與技巧 - Dropout</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day083_HW.ipynb>Day83_訓練神經網路的細節與技巧 - Batch normalization 因應 overfit 的方法概述 - 批次正規化 (Batch Normalization)</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day084_HW.ipynb>Day84_正規化/機移除/批次標準化的 組合與比較</a>** 
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day085_HW.ipynb>Day85_訓練神經網路的細節與技巧 - 使用 callbacks 函數做 earlystop 因應 overfit 的方法概述 - 煞車機制 (EarlyStopping)</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day086_HW.ipynb>Day86_訓練神經網路的細節與技巧 - 使用 callbacks 函數儲存 model 使用 Keras 內建的 callback 函數儲存訓練完的模型</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day087HW.ipynb>Day87_訓練神經網路的細節與技巧 - 使用 callbacks 函數做 reduce learning rate 使用 Keras 內建的 callback 函數做學習率遞減</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day088_HW.ipynb>Day88_訓練神經網路的細節與技巧 - 撰寫自己的 callbacks 函數</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day089_HW.ipynb>Day89_訓練神經網路的細節與技巧 - 撰寫自己的 Loss function</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day090_color_histogram_HW.ipynb>Day90_使用傳統電腦視覺與機器學習進行影像辨識</a>** : 了解在神經網路發展前，如何使用傳統機器學習演算法處理影像辨識
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day091_classification_with_cv_HW.ipynb>Day91_使用傳統電腦視覺與機器學習進行影像辨識</a>** : 應用傳統電腦視覺方法進行 CIFAR-10 分類


### 深度學習應用卷積神經網路
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day092_CNN_theory_HW.ipynb>Day92_卷積神經網路 (Convolution Neural Network, CNN) 簡介</a>** : 了解CNN的重要性, 以及CNN的組成結構
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day93-CNN_Brief_HW.ipynb>Day93_卷積神經網路架構細節</a>** : 為什麼比DNN更適合處理影像問題, 以及Keras上如何實作CNN
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day94-CNN_Convolution_HW.ipynb>Day94_卷積神經網路 - 卷積(Convolution)層與參數調整</a>** : 卷積層原理與參數說明
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day95-CNN_Pooling_Padding_HW.ipynb>Day95_卷積神經網路 - 池化(Pooling)層與參數調整</a>** : 介紹 Keras 中常用的 CNN layers
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day096_Keras_CNN_layers_HW.ipynb>Day96_Keras 中的 CNN layers</a>**
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day097_Keras_CNN_vs_DNN_HW.ipynb>Day97_使用 CNN 完成 CIFAR-10 資料集</a>** : 透過 CNN 訓練 CIFAR-10 並比較其與 DNN 的差異
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day098_Python_generator_HW.ipynb>Day98_訓練卷積神經網路的細節與技巧 - 處理大量數據</a>** : 資料無法放進記憶體該如何解決？如何使用 Python 的生成器 generator?
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day099_data_augmentation_HW.ipynb>Day99_訓練卷積神經網路的細節與技巧 - 處理小量數據</a>** : 資料太少準確率不高怎麼辦？如何使用資料增強提升準確率？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/Day100_transfer_learning_HW.ipynb>Day100_訓練卷積神經網路的細節與技巧 - 轉移學習 (Transfer learning)</a>** : 何謂轉移學習 Transfer learning？該如何使用？
- **<a href=https://github.com/PrestonYU/3rd-ML-100Days/blob/master/homework/D101-D103_Flower-Cognition_model4_HW.ipynb>Day101-103_Kaggle 期末考</a>** : 透過 Kaggle 影像辨識測驗, 綜合應用深度學習的課程內容, 並體驗遷移學習的威力

